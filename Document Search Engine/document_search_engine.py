# -*- coding: utf-8 -*-
"""Dokument Search Engine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yMt_Jm9Jfu9Ha7fmJ2ktIpr5H4XyvaEV

202010370311356 - Lina Khalisah
"""

pip install sastrawi

pip install transformers torch pandas

pip install nltk

import pandas as pd
import numpy as np
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from transformers import BertTokenizer, BertModel
import torch
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.metrics import confusion_matrix, classification_report

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import pairwise_distances

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('stopwords')
nltk.download('punkt')

df = pd.read_excel('Overview.xlsx')

df

df_copy = df.copy()

def preprocess_text(text):
    # Cek jika value adalah string
    if isinstance(text, str):
        # Menghilangkan angka
        text = ''.join([i for i in text if not i.isdigit()])

        # Proses case folding (lowercasing)
        text = text.lower()

        # Proses stemming
        factory = StemmerFactory()
        stemmer = factory.create_stemmer()
        text = stemmer.stem(text)

        # Proses stop word removal
        factory = StopWordRemoverFactory()
        stopword = factory.create_stop_word_remover()
        text = stopword.remove(text)

    return text

df_copy['BarangBukti_preprocessed'] = df_copy['Barang Bukti'].apply(preprocess_text)
df_copy['AmarPutusan_preprocessed'] = df_copy['Amar Putusan'].apply(preprocess_text)

df_copy

columns_to_drop = ['Barang Bukti', 'Amar Putusan']
df_copy = df_copy.drop(columns=columns_to_drop)
df_copy

df_copy.isna().sum()

df_new = df_copy.dropna()

df_new

"""#Model - Naive Bayes"""

# Fungsi untuk preprocessing teks
def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    stop_words = set(stopwords.words('indonesian'))
    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]
    return ' '.join(filtered_tokens)

# Preprocessing teks
df_new['BarangBukti_preprocessed'] = df_new['BarangBukti_preprocessed'].apply(preprocess_text)
df_new['AmarPutusan_preprocessed'] = df_new['AmarPutusan_preprocessed'].apply(preprocess_text)

df = df_new.dropna()

# Membuat model Naive Bayes untuk mengukur kemiripan
model = make_pipeline(TfidfVectorizer(), MultinomialNB())
model.fit(df['BarangBukti_preprocessed'] + ' ' + df['AmarPutusan_preprocessed'], df['No'])

# User input untuk query
user_query = input("Masukkan query: ")
user_query = preprocess_text(user_query)

# Transform the query using the TfidfVectorizer
query_vector = model.named_steps['tfidfvectorizer'].transform([user_query])

# Transform the documents in the dataset
document_vectors = model.named_steps['tfidfvectorizer'].transform(df['BarangBukti_preprocessed'] + ' ' + df['AmarPutusan_preprocessed'])

# Measure cosine similarity between the query and documents
similarities = cosine_similarity(query_vector, document_vectors)

# Find the indices of the top 5 documents with the highest similarity
top_indices = similarities.argsort()[0, ::-1][:5]

# Display the top 5 documents
print("\nTop 5 Documents:")
for i, index in enumerate(top_indices, 1):
    top_document = df.iloc[index]
    print(f"\nDocument {i}:")
    print("No Putusan:", top_document['No Putusan'])
    print("BarangBukti_preprocessed:", top_document['BarangBukti_preprocessed'])
    print("AmarPutusan_preprocessed:", top_document['AmarPutusan_preprocessed'])

# Model evaluation metrics
ground_truth_labels = df['No']
predicted_labels = model.predict(df['BarangBukti_preprocessed'] + ' ' + df['AmarPutusan_preprocessed'])

# Calculate and display evaluation metrics
accuracy = accuracy_score(ground_truth_labels, predicted_labels)
f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')  # Use 'weighted' for multiclass problems
recall = recall_score(ground_truth_labels, predicted_labels, average='weighted')
conf_matrix = confusion_matrix(ground_truth_labels, predicted_labels)

# Display the model evaluation metrics
print("\nModel Evaluation Metrics:")
print("Accuracy:", accuracy)
print("F1-score:", f1)
print("Recall:", recall)
print("Confusion Matrix:")
print(conf_matrix)